{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1098624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from konlpy.tag import Okt\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Lambda, Layer, LayerNormalization, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e2ab4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b47579fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11492\\1916856259.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mScaleDotProductAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_reduced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_reduced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;31m# linear projection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layer' is not defined"
     ]
    }
   ],
   "source": [
    "class ScaleDotProductAttention(layer):\n",
    "    def __init__(self, d_emb, d_reduced, masked=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.q = Dense(d_reduced, input_shape=(-1, d_emb))        # linear projection\n",
    "        self.k = Dense(d_reduced, input_shape=(-1, d_emb))        # linear projection\n",
    "        self.v = Dense(d_reduced, input_shape=(-1, d_emb))        # linear projection\n",
    "        \n",
    "        self.scaled = Lambda(lambda x: x/np.sqrt(d_reduced))    #1번 공식 활용\n",
    "        self.masked = masked\n",
    "    \n",
    "    def call(self, x, training=None, masked=None): # x shape = (q, k, v)\n",
    "        q = self.q(x[0])\n",
    "        k = self.k(x[1])\n",
    "        v = self.v(x[2])\n",
    "        \n",
    "        k_t = tf.transpose(k, perm=[0, 2, 1])      #[0,1,2] ->[0,2,1]로 transpose\n",
    "        product = tf.matmul(q, k_t)\n",
    "        \n",
    "        scaled = self.scaled(product)\n",
    "        \n",
    "        if masked:                                #add the mask\n",
    "            length = tf.shape(scaled)[-1]\n",
    "            mask = tf.fill((length, length), -np.inf)                   #inf = infinity\n",
    "            mask = tf.linalg.band_part(mask, 0, -1)                     # upper triangle\n",
    "            mask = tf.linalg.set_diag(mask, tf.zeros(length))\n",
    "            scaled += mask\n",
    "        \n",
    "        scaled = tf.nn.softmax(scaled, axis=-1)    #가장 낮은 차원에서 붙이기\n",
    "        \n",
    "        return tf.matmul(scaled, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3b69d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_emb, d_reduced, masked=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention_list = list()\n",
    "        \n",
    "        for _ in range(h):\n",
    "            self.attention_list.append(ScaledDotProductAttention(d_emb, d_reduced, masked)) #DotProduct에서 받아온 값들을 attentionlist에 저장\n",
    "        \n",
    "        self.linear = Dense(d_emb, input_shape=((-1, h * d_reduced)))\n",
    "                            \n",
    "    def call(self, x, training=None):\n",
    "        attention_list = [a(x) for a in self.attention_list]            #attention_list에 저장된 값들을 리스트로 만들어\n",
    "        concat = tf.concat(attention_list, axis=-1)                      # concat하기\n",
    "                            \n",
    "        return self.linear(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb4de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self, num_head, d_reduced):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.d_reduced = d_reduced\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.multihead_attention = MultiHeadAttention(self.num_head, input_shape[-1], self.d_reduced)\n",
    "        self.layer_normalization1 = LayerNormalization(input_shape=input_shape)\n",
    "        self.dense1 = Dense(input_shape[-1] * 4, input_shape=input_shape, activation='relu') #FNN에서 두 레이어 선형레이어 사이에 ReLU를 넣\n",
    "        self.dense2 = Dense(input_shape[-1])\n",
    "        self.layer_normalization2 = LayerNormalization(input_shape=input_shape)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, x, training=None, masked=None):\n",
    "        h = self.multihead_attention((x, x, x))\n",
    "        ln1 = self.layer_normalization1(x + h)\n",
    "        \n",
    "        h = self.dense2(self.dense1(ln1))\n",
    "        return self.layer_normalization2(ln1 + h)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3d1c45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Layer):\n",
    "    def __init__(self, num_head, d_reduced):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.d_reduced = d_reduced\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.self_attention = MultiHeadAttention(self.num_head, input_shape[0][-1], self.d_reduced, masked=True)\n",
    "        self.layer_normalization1 = LayerNormalization(input_shape=input_shape)\n",
    "        \n",
    "        self.multihead_attention = MultiHeadAttention(self.num_head, input_shape[0][-1], self.d_reduced)\n",
    "        self.layer_normalization2 = LayerNormalization(input_shape=input_shape)\n",
    "        \n",
    "        self.dense1 = Dense(input_shape[0][-1] * 4, input_shape=input_shape[0], activation='relu')\n",
    "        self.dense2 = Dense(input_shape[0][-1])\n",
    "        self.layer_normalization3 = LayerNormalization(input_shape=input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None, masked=None): #inputs (x, context)\n",
    "        x, context = inputs\n",
    "        h = self.self_attention((x, x, x))\n",
    "        ln1 = self.layer_normalization1(x + h)\n",
    "        \n",
    "        h = self.multihead_attention((ln1, context, context))\n",
    "        ln2 = self.layer_normalization2(ln1 + h)\n",
    "        \n",
    "        h = self.dense2(self.dense1(ln2))\n",
    "        return self.layer_normalization3(ln2 + h)\n",
    "        \n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e68f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(Layer):  # Referred from https://github.com/LastRemote/Transformer-TF2.0\n",
    "    def __init__(self, max_len, d_emb):\n",
    "        super().__init__()\n",
    "        self.sinusoidal_encoding = np.array([self.get_positional_angle(pos, d_emb) for pos in range(max_len)], dtype=np.float32)\n",
    "        self.sinusoidal_encoding[:, 0::2] = np.sin(self.sinusoidal_encoding[:, 0::2])  #sin 0~짝수\n",
    "        self.sinusoidal_encoding[:, 1::2] = np.cos(self.sinusoidal_encoding[:, 1::2])  #cos 1~홀수\n",
    "        self.sinusoidal_encoding = tf.cast(self.sinusoidal_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        return x + self.sinusoidal_encoding[:tf.shape(x)[1]]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_angle(self, pos, dim, d_emb):\n",
    "        return pos / np.power(10000, 2 * (dim // 2) / d_emb)\n",
    "\n",
    "    def get_positional_angle(self, pos, d_emb):\n",
    "        return [self.get_angle(pos, dim, d_emb) for dim in range(d_emb)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
